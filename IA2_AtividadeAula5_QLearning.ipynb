{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd50lmhCSAoHIKjhRNKjV6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardoDrLourenco/IA2_AtividadeAula5_QLearns/blob/main/IA2_AtividadeAula5_QLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fazer uma pesquisa sobre Q-Learning com redes neurais. Quais são as principais diferenças comparando com o modelo matemático apresentado?"
      ],
      "metadata": {
        "id": "ycSUhpLb19vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Q-Learning é uma técnica usada para ensinar um agente a tomar decisões com base nas suas experiências. Ele funciona armazenando uma tabela que diz qual é a melhor ação a tomar em cada situação. O problema é que essa tabela só funciona bem para problemas pequenos, porque quando o ambiente tem muitas opções, essa tabela fica gigantesca e difícil de lidar.\n",
        "\n",
        "Por outro lado, as redes neurais são ótimas para aprender padrões em grandes quantidades de dados. Elas podem substituir essa tabela do Q-Learning e fazer o agente tomar boas decisões mesmo em ambientes super complexos, como jogos de videogame ou problemas com muitas variáveis.\n",
        "\n",
        "Quando juntamos as duas ideias, temos o Deep Q-Learning (DQN). Nesse caso, a rede neural aprende a calcular as melhores ações a serem tomadas, o que faz com que o agente consiga lidar com problemas muito maiores e mais complicados do que o Q-Learning clássico sozinho.\n"
      ],
      "metadata": {
        "id": "Fth3nGbE2CZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principais diferenças:"
      ],
      "metadata": {
        "id": "cjpZWsyL2T9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representação dos estados e ações: No Q-Learning clássico, os estados e as ações são discretos e armazenados em uma tabela, enquanto no DQN, a rede neural aprende a mapear estados contínuos para valores Q.\n",
        "\n",
        "Escalabilidade: O DQN consegue lidar com ambientes complexos onde seria inviável armazenar todos os pares estado-ação. No Q-Learning clássico, isso é limitado pelo tamanho da tabela Q.\n",
        "\n",
        "Generalização: As redes neurais têm a capacidade de generalizar, o que significa que o agente pode aprender a tomar boas decisões mesmo em estados que não foram vistos diretamente durante o treinamento, algo difícil de se obter com a abordagem baseada em tabela.\n",
        "\n",
        "Instabilidade: O uso de redes neurais pode tornar o treinamento mais instável e demorado, necessitando de técnicas adicionais, como o replay de experiência e o uso de redes-alvo para estabilizar o aprendizado.\n"
      ],
      "metadata": {
        "id": "AAF_YCxK2WSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algoritmo Exemplo"
      ],
      "metadata": {
        "id": "Jo4W6S8x2tCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse código implementa um agente de aprendizado por reforço usando uma Rede Neural Profunda (DQN) para jogar o jogo CartPole (CartPole é um ambiente de aprendizado por reforço onde o objetivo é equilibrar um pêndulo invertido em um carrinho móvel). O agente tenta equilibrar um pêndulo em uma vara, aprendendo ao longo de várias tentativas. Ele utiliza uma memória de replay para armazenar experiências passadas e uma estratégia chamada epsilon-greedy para equilibrar a exploração de novas ações e a exploração de ações conhecidas. Com o tempo, o agente se torna mais habilidoso em manter o pêndulo equilibrado e maximizar suas recompensas."
      ],
      "metadata": {
        "id": "cGeAb2_R9sg_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92WKBlhz1jT8",
        "outputId": "ba7ef57e-6e0b-4d30-d2b1-08722b956027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (2.6.0)\n",
            "Episódio 0, Total de Recompensas: 25.0\n",
            "Episódio 1, Total de Recompensas: 19.0\n",
            "Episódio 2, Total de Recompensas: 10.0\n",
            "Episódio 3, Total de Recompensas: 12.0\n",
            "Episódio 4, Total de Recompensas: 11.0\n",
            "Episódio 5, Total de Recompensas: 13.0\n",
            "Episódio 6, Total de Recompensas: 28.0\n",
            "Episódio 7, Total de Recompensas: 18.0\n",
            "Episódio 8, Total de Recompensas: 10.0\n",
            "Episódio 9, Total de Recompensas: 11.0\n",
            "Episódio 10, Total de Recompensas: 36.0\n",
            "Episódio 11, Total de Recompensas: 27.0\n",
            "Episódio 12, Total de Recompensas: 38.0\n",
            "Episódio 13, Total de Recompensas: 29.0\n",
            "Episódio 14, Total de Recompensas: 44.0\n",
            "Episódio 15, Total de Recompensas: 19.0\n",
            "Episódio 16, Total de Recompensas: 17.0\n",
            "Episódio 17, Total de Recompensas: 41.0\n",
            "Episódio 18, Total de Recompensas: 14.0\n",
            "Episódio 19, Total de Recompensas: 21.0\n",
            "Episódio 20, Total de Recompensas: 15.0\n",
            "Episódio 21, Total de Recompensas: 10.0\n",
            "Episódio 22, Total de Recompensas: 18.0\n",
            "Episódio 23, Total de Recompensas: 12.0\n",
            "Episódio 24, Total de Recompensas: 15.0\n",
            "Episódio 25, Total de Recompensas: 13.0\n",
            "Episódio 26, Total de Recompensas: 19.0\n",
            "Episódio 27, Total de Recompensas: 26.0\n",
            "Episódio 28, Total de Recompensas: 34.0\n",
            "Episódio 29, Total de Recompensas: 15.0\n",
            "Episódio 30, Total de Recompensas: 21.0\n",
            "Episódio 31, Total de Recompensas: 28.0\n",
            "Episódio 32, Total de Recompensas: 43.0\n",
            "Episódio 33, Total de Recompensas: 63.0\n",
            "Episódio 34, Total de Recompensas: 20.0\n",
            "Episódio 35, Total de Recompensas: 11.0\n",
            "Episódio 36, Total de Recompensas: 14.0\n",
            "Episódio 37, Total de Recompensas: 16.0\n",
            "Episódio 38, Total de Recompensas: 69.0\n",
            "Episódio 39, Total de Recompensas: 53.0\n",
            "Episódio 40, Total de Recompensas: 18.0\n",
            "Episódio 41, Total de Recompensas: 10.0\n",
            "Episódio 42, Total de Recompensas: 13.0\n",
            "Episódio 43, Total de Recompensas: 20.0\n",
            "Episódio 44, Total de Recompensas: 18.0\n",
            "Episódio 45, Total de Recompensas: 15.0\n",
            "Episódio 46, Total de Recompensas: 43.0\n",
            "Episódio 47, Total de Recompensas: 32.0\n",
            "Episódio 48, Total de Recompensas: 36.0\n",
            "Episódio 49, Total de Recompensas: 18.0\n",
            "Episódio 50, Total de Recompensas: 42.0\n",
            "Episódio 51, Total de Recompensas: 24.0\n",
            "Episódio 52, Total de Recompensas: 23.0\n",
            "Episódio 53, Total de Recompensas: 23.0\n",
            "Episódio 54, Total de Recompensas: 12.0\n",
            "Episódio 55, Total de Recompensas: 48.0\n",
            "Episódio 56, Total de Recompensas: 12.0\n",
            "Episódio 57, Total de Recompensas: 20.0\n",
            "Episódio 58, Total de Recompensas: 16.0\n",
            "Episódio 59, Total de Recompensas: 64.0\n",
            "Episódio 60, Total de Recompensas: 56.0\n",
            "Episódio 61, Total de Recompensas: 21.0\n",
            "Episódio 62, Total de Recompensas: 36.0\n",
            "Episódio 63, Total de Recompensas: 74.0\n",
            "Episódio 64, Total de Recompensas: 34.0\n",
            "Episódio 65, Total de Recompensas: 39.0\n",
            "Episódio 66, Total de Recompensas: 41.0\n",
            "Episódio 67, Total de Recompensas: 14.0\n",
            "Episódio 68, Total de Recompensas: 33.0\n",
            "Episódio 69, Total de Recompensas: 74.0\n",
            "Episódio 70, Total de Recompensas: 21.0\n",
            "Episódio 71, Total de Recompensas: 13.0\n",
            "Episódio 72, Total de Recompensas: 17.0\n",
            "Episódio 73, Total de Recompensas: 79.0\n",
            "Episódio 74, Total de Recompensas: 28.0\n",
            "Episódio 75, Total de Recompensas: 65.0\n",
            "Episódio 76, Total de Recompensas: 14.0\n",
            "Episódio 77, Total de Recompensas: 96.0\n",
            "Episódio 78, Total de Recompensas: 147.0\n",
            "Episódio 79, Total de Recompensas: 87.0\n",
            "Episódio 80, Total de Recompensas: 108.0\n",
            "Episódio 81, Total de Recompensas: 38.0\n",
            "Episódio 82, Total de Recompensas: 26.0\n",
            "Episódio 83, Total de Recompensas: 16.0\n",
            "Episódio 84, Total de Recompensas: 100.0\n",
            "Episódio 85, Total de Recompensas: 97.0\n",
            "Episódio 86, Total de Recompensas: 70.0\n",
            "Episódio 87, Total de Recompensas: 71.0\n",
            "Episódio 88, Total de Recompensas: 32.0\n",
            "Episódio 89, Total de Recompensas: 102.0\n",
            "Episódio 90, Total de Recompensas: 68.0\n",
            "Episódio 91, Total de Recompensas: 42.0\n",
            "Episódio 92, Total de Recompensas: 17.0\n",
            "Episódio 93, Total de Recompensas: 173.0\n",
            "Episódio 94, Total de Recompensas: 18.0\n",
            "Episódio 95, Total de Recompensas: 31.0\n",
            "Episódio 96, Total de Recompensas: 21.0\n",
            "Episódio 97, Total de Recompensas: 36.0\n",
            "Episódio 98, Total de Recompensas: 214.0\n",
            "Episódio 99, Total de Recompensas: 76.0\n",
            "Episódio 100, Total de Recompensas: 134.0\n",
            "Episódio 101, Total de Recompensas: 76.0\n",
            "Episódio 102, Total de Recompensas: 17.0\n",
            "Episódio 103, Total de Recompensas: 22.0\n",
            "Episódio 104, Total de Recompensas: 31.0\n",
            "Episódio 105, Total de Recompensas: 101.0\n",
            "Episódio 106, Total de Recompensas: 66.0\n",
            "Episódio 107, Total de Recompensas: 160.0\n",
            "Episódio 108, Total de Recompensas: 13.0\n",
            "Episódio 109, Total de Recompensas: 138.0\n",
            "Episódio 110, Total de Recompensas: 195.0\n",
            "Episódio 111, Total de Recompensas: 119.0\n",
            "Episódio 112, Total de Recompensas: 136.0\n",
            "Episódio 113, Total de Recompensas: 80.0\n",
            "Episódio 114, Total de Recompensas: 103.0\n",
            "Episódio 115, Total de Recompensas: 66.0\n",
            "Episódio 116, Total de Recompensas: 170.0\n",
            "Episódio 117, Total de Recompensas: 99.0\n",
            "Episódio 118, Total de Recompensas: 216.0\n",
            "Episódio 119, Total de Recompensas: 52.0\n",
            "Episódio 120, Total de Recompensas: 288.0\n",
            "Episódio 121, Total de Recompensas: 128.0\n",
            "Episódio 122, Total de Recompensas: 185.0\n",
            "Episódio 123, Total de Recompensas: 56.0\n",
            "Episódio 124, Total de Recompensas: 110.0\n",
            "Episódio 125, Total de Recompensas: 208.0\n",
            "Episódio 126, Total de Recompensas: 411.0\n",
            "Episódio 127, Total de Recompensas: 329.0\n",
            "Episódio 128, Total de Recompensas: 73.0\n",
            "Episódio 129, Total de Recompensas: 324.0\n",
            "Episódio 130, Total de Recompensas: 408.0\n",
            "Episódio 131, Total de Recompensas: 10.0\n",
            "Episódio 132, Total de Recompensas: 119.0\n",
            "Episódio 133, Total de Recompensas: 109.0\n",
            "Episódio 134, Total de Recompensas: 74.0\n",
            "Episódio 135, Total de Recompensas: 74.0\n",
            "Episódio 136, Total de Recompensas: 253.0\n",
            "Episódio 137, Total de Recompensas: 46.0\n",
            "Episódio 138, Total de Recompensas: 54.0\n",
            "Episódio 139, Total de Recompensas: 201.0\n",
            "Episódio 140, Total de Recompensas: 26.0\n",
            "Episódio 141, Total de Recompensas: 30.0\n",
            "Episódio 142, Total de Recompensas: 254.0\n",
            "Episódio 143, Total de Recompensas: 116.0\n",
            "Episódio 144, Total de Recompensas: 129.0\n",
            "Episódio 145, Total de Recompensas: 188.0\n",
            "Episódio 146, Total de Recompensas: 38.0\n",
            "Episódio 147, Total de Recompensas: 130.0\n",
            "Episódio 148, Total de Recompensas: 292.0\n",
            "Episódio 149, Total de Recompensas: 67.0\n",
            "Episódio 150, Total de Recompensas: 153.0\n",
            "Episódio 151, Total de Recompensas: 232.0\n",
            "Episódio 152, Total de Recompensas: 138.0\n",
            "Episódio 153, Total de Recompensas: 165.0\n",
            "Episódio 154, Total de Recompensas: 130.0\n",
            "Episódio 155, Total de Recompensas: 95.0\n",
            "Episódio 156, Total de Recompensas: 16.0\n",
            "Episódio 157, Total de Recompensas: 323.0\n",
            "Episódio 158, Total de Recompensas: 86.0\n",
            "Episódio 159, Total de Recompensas: 237.0\n",
            "Episódio 160, Total de Recompensas: 236.0\n",
            "Episódio 161, Total de Recompensas: 88.0\n",
            "Episódio 162, Total de Recompensas: 139.0\n",
            "Episódio 163, Total de Recompensas: 112.0\n",
            "Episódio 164, Total de Recompensas: 94.0\n",
            "Episódio 165, Total de Recompensas: 253.0\n",
            "Episódio 166, Total de Recompensas: 148.0\n",
            "Episódio 167, Total de Recompensas: 99.0\n",
            "Episódio 168, Total de Recompensas: 242.0\n",
            "Episódio 169, Total de Recompensas: 246.0\n",
            "Episódio 170, Total de Recompensas: 168.0\n",
            "Episódio 171, Total de Recompensas: 306.0\n",
            "Episódio 172, Total de Recompensas: 133.0\n",
            "Episódio 173, Total de Recompensas: 15.0\n",
            "Episódio 174, Total de Recompensas: 96.0\n",
            "Episódio 175, Total de Recompensas: 244.0\n",
            "Episódio 176, Total de Recompensas: 190.0\n",
            "Episódio 177, Total de Recompensas: 16.0\n",
            "Episódio 178, Total de Recompensas: 235.0\n",
            "Episódio 179, Total de Recompensas: 347.0\n",
            "Episódio 180, Total de Recompensas: 354.0\n",
            "Episódio 181, Total de Recompensas: 155.0\n",
            "Episódio 182, Total de Recompensas: 360.0\n",
            "Episódio 183, Total de Recompensas: 249.0\n",
            "Episódio 184, Total de Recompensas: 378.0\n",
            "Episódio 185, Total de Recompensas: 267.0\n",
            "Episódio 186, Total de Recompensas: 364.0\n",
            "Episódio 187, Total de Recompensas: 304.0\n",
            "Episódio 188, Total de Recompensas: 106.0\n",
            "Episódio 189, Total de Recompensas: 359.0\n",
            "Episódio 190, Total de Recompensas: 272.0\n",
            "Episódio 191, Total de Recompensas: 121.0\n",
            "Episódio 192, Total de Recompensas: 480.0\n",
            "Episódio 193, Total de Recompensas: 29.0\n",
            "Episódio 194, Total de Recompensas: 337.0\n",
            "Episódio 195, Total de Recompensas: 140.0\n",
            "Episódio 196, Total de Recompensas: 302.0\n",
            "Episódio 197, Total de Recompensas: 312.0\n",
            "Episódio 198, Total de Recompensas: 372.0\n",
            "Episódio 199, Total de Recompensas: 308.0\n",
            "Episódio 200, Total de Recompensas: 292.0\n",
            "Episódio 201, Total de Recompensas: 25.0\n",
            "Episódio 202, Total de Recompensas: 397.0\n",
            "Episódio 203, Total de Recompensas: 162.0\n",
            "Episódio 204, Total de Recompensas: 283.0\n",
            "Episódio 205, Total de Recompensas: 112.0\n",
            "Episódio 206, Total de Recompensas: 342.0\n",
            "Episódio 207, Total de Recompensas: 307.0\n",
            "Episódio 208, Total de Recompensas: 393.0\n",
            "Episódio 209, Total de Recompensas: 24.0\n",
            "Episódio 210, Total de Recompensas: 683.0\n",
            "Episódio 211, Total de Recompensas: 330.0\n",
            "Episódio 212, Total de Recompensas: 324.0\n",
            "Episódio 213, Total de Recompensas: 236.0\n",
            "Episódio 214, Total de Recompensas: 211.0\n",
            "Episódio 215, Total de Recompensas: 318.0\n",
            "Episódio 216, Total de Recompensas: 577.0\n",
            "Episódio 217, Total de Recompensas: 136.0\n",
            "Episódio 218, Total de Recompensas: 213.0\n",
            "Episódio 219, Total de Recompensas: 457.0\n",
            "Episódio 220, Total de Recompensas: 275.0\n",
            "Episódio 221, Total de Recompensas: 346.0\n",
            "Episódio 222, Total de Recompensas: 259.0\n",
            "Episódio 223, Total de Recompensas: 225.0\n",
            "Episódio 224, Total de Recompensas: 279.0\n",
            "Episódio 225, Total de Recompensas: 218.0\n",
            "Episódio 226, Total de Recompensas: 59.0\n",
            "Episódio 227, Total de Recompensas: 323.0\n",
            "Episódio 228, Total de Recompensas: 228.0\n",
            "Episódio 229, Total de Recompensas: 371.0\n",
            "Episódio 230, Total de Recompensas: 421.0\n",
            "Episódio 231, Total de Recompensas: 293.0\n",
            "Episódio 232, Total de Recompensas: 205.0\n",
            "Episódio 233, Total de Recompensas: 255.0\n",
            "Episódio 234, Total de Recompensas: 103.0\n",
            "Episódio 235, Total de Recompensas: 245.0\n",
            "Episódio 236, Total de Recompensas: 198.0\n",
            "Episódio 237, Total de Recompensas: 189.0\n",
            "Episódio 238, Total de Recompensas: 356.0\n",
            "Episódio 239, Total de Recompensas: 216.0\n",
            "Episódio 240, Total de Recompensas: 151.0\n",
            "Episódio 241, Total de Recompensas: 13.0\n",
            "Episódio 242, Total de Recompensas: 54.0\n",
            "Episódio 243, Total de Recompensas: 112.0\n",
            "Episódio 244, Total de Recompensas: 130.0\n",
            "Episódio 245, Total de Recompensas: 120.0\n",
            "Episódio 246, Total de Recompensas: 123.0\n",
            "Episódio 247, Total de Recompensas: 132.0\n",
            "Episódio 248, Total de Recompensas: 125.0\n",
            "Episódio 249, Total de Recompensas: 121.0\n",
            "Episódio 250, Total de Recompensas: 17.0\n",
            "Episódio 251, Total de Recompensas: 145.0\n",
            "Episódio 252, Total de Recompensas: 43.0\n",
            "Episódio 253, Total de Recompensas: 154.0\n",
            "Episódio 254, Total de Recompensas: 146.0\n",
            "Episódio 255, Total de Recompensas: 148.0\n",
            "Episódio 256, Total de Recompensas: 78.0\n",
            "Episódio 257, Total de Recompensas: 129.0\n",
            "Episódio 258, Total de Recompensas: 111.0\n",
            "Episódio 259, Total de Recompensas: 142.0\n",
            "Episódio 260, Total de Recompensas: 156.0\n",
            "Episódio 261, Total de Recompensas: 164.0\n",
            "Episódio 262, Total de Recompensas: 53.0\n",
            "Episódio 263, Total de Recompensas: 155.0\n",
            "Episódio 264, Total de Recompensas: 143.0\n",
            "Episódio 265, Total de Recompensas: 138.0\n",
            "Episódio 266, Total de Recompensas: 19.0\n",
            "Episódio 267, Total de Recompensas: 149.0\n",
            "Episódio 268, Total de Recompensas: 21.0\n",
            "Episódio 269, Total de Recompensas: 84.0\n",
            "Episódio 270, Total de Recompensas: 29.0\n",
            "Episódio 271, Total de Recompensas: 131.0\n",
            "Episódio 272, Total de Recompensas: 34.0\n",
            "Episódio 273, Total de Recompensas: 132.0\n",
            "Episódio 274, Total de Recompensas: 67.0\n",
            "Episódio 275, Total de Recompensas: 88.0\n",
            "Episódio 276, Total de Recompensas: 83.0\n",
            "Episódio 277, Total de Recompensas: 133.0\n",
            "Episódio 278, Total de Recompensas: 130.0\n",
            "Episódio 279, Total de Recompensas: 24.0\n",
            "Episódio 280, Total de Recompensas: 148.0\n",
            "Episódio 281, Total de Recompensas: 11.0\n",
            "Episódio 282, Total de Recompensas: 121.0\n",
            "Episódio 283, Total de Recompensas: 22.0\n",
            "Episódio 284, Total de Recompensas: 79.0\n",
            "Episódio 285, Total de Recompensas: 120.0\n",
            "Episódio 286, Total de Recompensas: 12.0\n",
            "Episódio 287, Total de Recompensas: 118.0\n",
            "Episódio 288, Total de Recompensas: 120.0\n",
            "Episódio 289, Total de Recompensas: 117.0\n",
            "Episódio 290, Total de Recompensas: 109.0\n",
            "Episódio 291, Total de Recompensas: 13.0\n",
            "Episódio 292, Total de Recompensas: 112.0\n",
            "Episódio 293, Total de Recompensas: 120.0\n",
            "Episódio 294, Total de Recompensas: 108.0\n",
            "Episódio 295, Total de Recompensas: 15.0\n",
            "Episódio 296, Total de Recompensas: 33.0\n",
            "Episódio 297, Total de Recompensas: 115.0\n",
            "Episódio 298, Total de Recompensas: 117.0\n",
            "Episódio 299, Total de Recompensas: 119.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[classic_control]  # Instala o Gym (fork do OpenAI Gym)\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Definindo a rede neural DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Memória de Replay\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return np.vstack(states), actions, rewards, np.vstack(next_states), dones\n",
        "\n",
        "# Função de ação com epsilon-greedy\n",
        "def act(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = agent(state)\n",
        "        return np.argmax(q_values.detach().numpy())\n",
        "\n",
        "# Parâmetros\n",
        "state_size = 4  # Tamanho do vetor de estado (CartPole)\n",
        "action_size = 2  # Número de ações (esquerda ou direita)\n",
        "batch_size = 32\n",
        "gamma = 0.99  # Fator de desconto\n",
        "epsilon = 1.0  # Taxa de exploração inicial\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "learning_rate = 0.001\n",
        "target_update = 10  # Frequência de atualização da rede alvo\n",
        "\n",
        "# Configuração do ambiente\n",
        "env = gym.make('CartPole-v1')\n",
        "agent = DQN(state_size, action_size)\n",
        "target_net = DQN(state_size, action_size)  # Rede alvo\n",
        "target_net.load_state_dict(agent.state_dict())\n",
        "optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "memory = ReplayBuffer(10000)\n",
        "\n",
        "# Treinamento\n",
        "episodes = 300\n",
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = act(state, epsilon)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        memory.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # Atualizar a rede neural\n",
        "        if len(memory.buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
        "            states = torch.FloatTensor(states)\n",
        "            next_states = torch.FloatTensor(next_states)\n",
        "            rewards = torch.FloatTensor(rewards)\n",
        "            dones = torch.FloatTensor(dones)\n",
        "\n",
        "            q_values = agent(states).gather(1, torch.LongTensor(actions).unsqueeze(1)).squeeze(1)\n",
        "            next_q_values = target_net(next_states).max(1)[0]\n",
        "            targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "            loss = (q_values - targets).pow(2).mean()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Atualizar a rede alvo\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(agent.state_dict())\n",
        "\n",
        "    # Reduzir epsilon (exploração)\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    print(f'Episódio {episode}, Total de Recompensas: {total_reward}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os resultados mostram que o agente de aprendizado por reforço no CartPole está passando por altos e baixos nas recompensas. Embora algumas recompensas sejam bem baixas, parece que, no geral, ele está aprendendo e melhorando com o tempo."
      ],
      "metadata": {
        "id": "nPzatS7V-Ta3"
      }
    }
  ]
}